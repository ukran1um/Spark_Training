{"metadata": {"language_info": {"file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython2", "name": "python", "version": "2.7.11", "codemirror_mode": {"version": 2, "name": "ipython"}}, "kernelspec": {"display_name": "Python 2", "language": "python", "name": "python2"}}, "nbformat": 4, "cells": [{"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "0.0\n1.0\n0.0\n", "name": "stdout"}], "cell_type": "code", "execution_count": 1, "source": "from pyspark.ml.feature import Binarizer\n\ncontinuousDataFrame = sqlContext.createDataFrame([\n    (0, 0.1),\n    (1, 0.8),\n    (2, 0.2)\n], [\"label\", \"feature\"])\nbinarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\nbinarizedDataFrame = binarizer.transform(continuousDataFrame)\nbinarizedFeatures = binarizedDataFrame.select(\"binarized_feature\")\nfor binarized_feature, in binarizedFeatures.collect():\n    print(binarized_feature)"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "+-----------------------------------------------------------+\n|pcaFeatures                                                |\n+-----------------------------------------------------------+\n|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n+-----------------------------------------------------------+\n\n", "name": "stdout"}], "cell_type": "code", "execution_count": 2, "source": "from pyspark.ml.feature import PCA\nfrom pyspark.mllib.linalg import Vectors\n\ndata = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\ndf = sqlContext.createDataFrame(data, [\"features\"])\npca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\nmodel = pca.fit(df)\nresult = model.transform(df).select(\"pcaFeatures\")\nresult.show(truncate=False)"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "Row(polyFeatures=DenseVector([-2.0, 4.0, 2.3, -4.6, 5.29]))\nRow(polyFeatures=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0]))\nRow(polyFeatures=DenseVector([0.6, 0.36, -1.1, -0.66, 1.21]))\n", "name": "stdout"}], "cell_type": "code", "execution_count": 3, "source": "from pyspark.ml.feature import PolynomialExpansion\nfrom pyspark.mllib.linalg import Vectors\n\ndf = sqlContext\\\n    .createDataFrame([(Vectors.dense([-2.0, 2.3]),),\n                      (Vectors.dense([0.0, 0.0]),),\n                      (Vectors.dense([0.6, -1.1]),)],\n                     [\"features\"])\npx = PolynomialExpansion(degree=2, inputCol=\"features\", outputCol=\"polyFeatures\")\npolyDF = px.transform(df)\nfor expanded in polyDF.select(\"polyFeatures\").take(3):\n    print(expanded)"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "+---+-------------+\n| id|  categoryVec|\n+---+-------------+\n|  0|(3,[0],[1.0])|\n|  1|(3,[2],[1.0])|\n|  2|(3,[1],[1.0])|\n|  3|(3,[0],[1.0])|\n|  4|(3,[0],[1.0])|\n|  5|(3,[1],[1.0])|\n+---+-------------+\n\n", "name": "stdout"}], "cell_type": "code", "execution_count": 4, "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n\ndf = sqlContext.createDataFrame([\n    (0, \"a\"),\n    (1, \"b\"),\n    (2, \"c\"),\n    (3, \"a\"),\n    (4, \"a\"),\n    (5, \"c\")\n], [\"id\", \"category\"])\n\nstringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\nmodel = stringIndexer.fit(df)\nindexed = model.transform(df)\nencoder = OneHotEncoder(dropLast=False, inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\nencoded = encoder.transform(indexed)\nencoded.select(\"id\", \"categoryVec\").show()"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0\n", "name": "stdout"}], "cell_type": "code", "execution_count": 15, "source": "print os.environ['SPARK_HOME']"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "['als', 'gmm_data.txt', 'kmeans_data.txt', 'lr-data', 'lr_data.txt', 'pagerank_data.txt', 'pic_data.txt', 'ridge-data', 'sample_binary_classification_data.txt', 'sample_fpgrowth.txt', 'sample_isotonic_regression_data.txt', 'sample_lda_data.txt', 'sample_libsvm_data.txt', 'sample_linear_regression_data.txt', 'sample_movielens_data.txt', 'sample_multiclass_classification_data.txt', 'sample_naive_bayes_data.txt', 'sample_svm_data.txt', 'sample_tree_data.csv']\n", "name": "stdout"}], "cell_type": "code", "execution_count": 25, "source": "print os.listdir(\"/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/data/mllib\")"}, {"metadata": {"collapsed": true}, "outputs": [], "cell_type": "code", "execution_count": 26, "source": "from pyspark.ml.feature import Normalizer\n\ndataFrame = sqlContext.read.format(\"libsvm\").load(\"/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/data/mllib/sample_libsvm_data.txt\")"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "+-----+--------------------+--------------------+\n|label|            features|        normFeatures|\n+-----+--------------------+--------------------+\n|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n+-----+--------------------+--------------------+\nonly showing top 20 rows\n\n", "name": "stdout"}], "cell_type": "code", "execution_count": 27, "source": "# Normalize each Vector using $L^1$ norm.\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\nl1NormData = normalizer.transform(dataFrame)\nl1NormData.show()"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "+-----+--------------------+--------------------+\n|label|            features|      scaledFeatures|\n+-----+--------------------+--------------------+\n|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n+-----+--------------------+--------------------+\nonly showing top 20 rows\n\n", "name": "stdout"}], "cell_type": "code", "execution_count": 28, "source": "from pyspark.ml.feature import StandardScaler\n\ndataFrame = sqlContext.read.format(\"libsvm\").load(\"/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/data/mllib/sample_libsvm_data.txt\")\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n                        withStd=True, withMean=False)\n\n# Compute summary statistics by fitting the StandardScaler\nscalerModel = scaler.fit(dataFrame)\n\n# Normalize each feature to have unit standard deviation.\nscaledData = scalerModel.transform(dataFrame)\nscaledData.show()"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "+-----+--------------------+--------------------+\n|label|            features|      scaledFeatures|\n+-----+--------------------+--------------------+\n|  0.0|(692,[127,128,129...|[0.5,0.5,0.5,0.5,...|\n|  1.0|(692,[158,159,160...|[0.5,0.5,0.5,0.5,...|\n|  1.0|(692,[124,125,126...|[0.5,0.5,0.5,0.5,...|\n|  1.0|(692,[152,153,154...|[0.5,0.5,0.5,0.5,...|\n|  1.0|(692,[151,152,153...|[0.5,0.5,0.5,0.5,...|\n|  0.0|(692,[129,130,131...|[0.5,0.5,0.5,0.5,...|\n|  1.0|(692,[158,159,160...|[0.5,0.5,0.5,0.5,...|\n|  1.0|(692,[99,100,101,...|[0.5,0.5,0.5,0.5,...|\n|  0.0|(692,[154,155,156...|[0.5,0.5,0.5,0.5,...|\n|  0.0|(692,[127,128,129...|[0.5,0.5,0.5,0.5,...|\n|  1.0|(692,[154,155,156...|[0.5,0.5,0.5,0.5,...|\n|  0.0|(692,[153,154,155...|[0.5,0.5,0.5,0.5,...|\n|  0.0|(692,[151,152,153...|[0.5,0.5,0.5,0.5,...|\n|  1.0|(692,[129,130,131...|[0.5,0.5,0.5,0.5,...|\n|  0.0|(692,[154,155,156...|[0.5,0.5,0.5,0.5,...|\n|  1.0|(692,[150,151,152...|[0.5,0.5,0.5,0.5,...|\n|  0.0|(692,[124,125,126...|[0.5,0.5,0.5,0.5,...|\n|  0.0|(692,[152,153,154...|[0.5,0.5,0.5,0.5,...|\n|  1.0|(692,[97,98,99,12...|[0.5,0.5,0.5,0.5,...|\n|  1.0|(692,[124,125,126...|[0.5,0.5,0.5,0.5,...|\n+-----+--------------------+--------------------+\nonly showing top 20 rows\n\n", "name": "stdout"}], "cell_type": "code", "execution_count": 30, "source": "from pyspark.ml.feature import MinMaxScaler\n\ndataFrame = sqlContext.read.format(\"libsvm\").load(\"/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/data/mllib/sample_libsvm_data.txt\")\n\nscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n\n# Compute summary statistics and generate MinMaxScalerModel\nscalerModel = scaler.fit(dataFrame)\n\n# rescale each feature to range [min, max].\nscaledData = scalerModel.transform(dataFrame)\nscaledData.show()"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "+---+---+---+---+----+\n| id| v1| v2| v3|  v4|\n+---+---+---+---+----+\n|  0|1.0|3.0|4.0| 3.0|\n|  2|2.0|5.0|7.0|10.0|\n+---+---+---+---+----+\n\n", "name": "stdout"}], "cell_type": "code", "execution_count": 31, "source": "from pyspark.ml.feature import SQLTransformer\n\ndf = sqlContext.createDataFrame([\n    (0, 1.0, 3.0),\n    (2, 2.0, 5.0)\n], [\"id\", \"v1\", \"v2\"])\nsqlTrans = SQLTransformer(\n    statement=\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\")\nsqlTrans.transform(df).show()"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "+--------+----------------+\n|features|bucketedFeatures|\n+--------+----------------+\n|    -0.5|             1.0|\n|    -0.3|             1.0|\n|     0.0|             2.0|\n|     0.2|             2.0|\n+--------+----------------+\n\n", "name": "stdout"}], "cell_type": "code", "execution_count": 33, "source": "from pyspark.ml.feature import Bucketizer\n\nsplits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n\ndata = [(-0.5,), (-0.3,), (0.0,), (0.2,)]\ndataFrame = sqlContext.createDataFrame(data, [\"features\"])\n\nbucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n\n# Transform original data into its bucket index.\nbucketedData = bucketizer.transform(dataFrame)\nbucketedData.show()"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "+-------------+-----------------+\n|       vector|transformedVector|\n+-------------+-----------------+\n|[1.0,2.0,3.0]|    [0.0,2.0,6.0]|\n|[4.0,5.0,6.0]|   [0.0,5.0,12.0]|\n+-------------+-----------------+\n\n", "name": "stdout"}], "cell_type": "code", "execution_count": 34, "source": "from pyspark.ml.feature import ElementwiseProduct\nfrom pyspark.mllib.linalg import Vectors\n\ndata = [(Vectors.dense([1.0, 2.0, 3.0]),), (Vectors.dense([4.0, 5.0, 6.0]),)]\ndf = sqlContext.createDataFrame(data, [\"vector\"])\ntransformer = ElementwiseProduct(scalingVec=Vectors.dense([0.0, 1.0, 2.0]),\n                                 inputCol=\"vector\", outputCol=\"transformedVector\")\ntransformer.transform(df).show()"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "Row(features=DenseVector([18.0, 1.0, 0.0, 10.0, 0.5]), clicked=1.0)\n", "name": "stdout"}], "cell_type": "code", "execution_count": 35, "source": "from pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\ndataset = sqlContext.createDataFrame(\n    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\nassembler = VectorAssembler(\n    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n    outputCol=\"features\")\noutput = assembler.transform(dataset)\nprint(output.select(\"features\", \"clicked\").first())"}, {"metadata": {"collapsed": false}, "outputs": [{"output_type": "stream", "text": "+--------------+-----+\n|      features|label|\n+--------------+-----+\n|[0.0,0.0,18.0]|  1.0|\n|[1.0,0.0,12.0]|  0.0|\n|[0.0,1.0,15.0]|  0.0|\n+--------------+-----+\n\n", "name": "stdout"}], "cell_type": "code", "execution_count": 38, "source": "from pyspark.ml.feature import RFormula\n\ndataset = sqlContext.createDataFrame(\n    [(7, \"US\", 18, 1.0),\n     (8, \"CA\", 12, 0.0),\n     (9, \"NZ\", 15, 0.0)],\n    [\"id\", \"country\", \"hour\", \"clicked\"])\nformula = RFormula(\n    formula=\"clicked ~ country + hour\",\n    featuresCol=\"features\",\n    labelCol=\"label\")\noutput = formula.fit(dataset).transform(dataset)\noutput.select(\"features\", \"label\").show()"}, {"metadata": {"collapsed": true}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ""}], "nbformat_minor": 0}