{"nbformat_minor": 0, "cells": [{"source": "input = os.environ['SPARK_HOME'] + \"/data/mllib/sample_svm_data.txt\"\nmymodelpath = os.getcwd() + \"/test/modelpath\"\nprint input", "metadata": {"collapsed": false}, "execution_count": 1, "cell_type": "code", "outputs": [{"name": "stdout", "text": "/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/data/mllib/sample_svm_data.txt\n", "output_type": "stream"}]}, {"source": "# MLLIB approach\nfrom pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\nfrom pyspark.mllib.regression import LabeledPoint", "metadata": {"collapsed": true}, "execution_count": 2, "cell_type": "code", "outputs": []}, {"source": "# Load and parse the data\ndef parsePoint(line):\n    values = [float(x) for x in line.split(' ')]\n    return LabeledPoint(values[0], values[1:])", "metadata": {"collapsed": true}, "execution_count": 3, "cell_type": "code", "outputs": []}, {"source": "data = sc.textFile(input)\nparsedData = data.map(parsePoint)", "metadata": {"collapsed": true}, "execution_count": 4, "cell_type": "code", "outputs": []}, {"source": "# Build the model\nmodel = LogisticRegressionWithLBFGS.train(parsedData)", "metadata": {"collapsed": true}, "execution_count": 5, "cell_type": "code", "outputs": []}, {"source": "# Evaluating the model on training data\nlabelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\ntrainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())\nprint(\"Training Error = \" + str(trainErr))", "metadata": {"collapsed": false}, "execution_count": 6, "cell_type": "code", "outputs": [{"name": "stdout", "text": "Training Error = 0.366459627329\n", "output_type": "stream"}]}, {"source": "input = os.environ['SPARK_HOME'] + \"/data/mllib/sample_libsvm_data.txt\"\nprint input", "metadata": {"collapsed": false}, "execution_count": 10, "cell_type": "code", "outputs": [{"name": "stdout", "text": "/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/data/mllib/sample_libsvm_data.txt\n", "output_type": "stream"}]}, {"source": "from pyspark.ml.classification import LogisticRegression\n\n# Load training data\ntraining = sqlContext.read.format(\"libsvm\").load(input)\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)", "metadata": {"collapsed": true}, "execution_count": 11, "cell_type": "code", "outputs": []}, {"source": "# Fit the model\nlrModel = lr.fit(training)\n\n# Print the coefficients and intercept for logistic regression\nprint(\"Coefficients: \" + str(lrModel.coefficients))\nprint(\"Intercept: \" + str(lrModel.intercept))", "metadata": {"collapsed": false}, "execution_count": 12, "cell_type": "code", "outputs": [{"name": "stdout", "text": "Coefficients: (692,[244,263,272,300,301,328,350,351,378,379,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.35398352419e-05,-9.10273850559e-05,-0.000194674305469,-0.000203006424735,-3.14761833149e-05,-6.84297760266e-05,1.58836268982e-05,1.40234970914e-05,0.00035432047525,0.000114432728982,0.000100167123837,0.00060141093038,0.000284024817912,-0.000115410847365,0.000385996886313,0.000635019557424,-0.000115064123846,-0.00015271865865,0.000280493380899,0.000607011747119,-0.000200845966325,-0.000142107557929,0.000273901034116,0.00027730456245,-9.83802702727e-05,-0.000380852244352,-0.000253151980086,0.000277477147708,-0.000244361976392,-0.00153947446876,-0.000230733284113])\nIntercept: 0.224563159613\n", "output_type": "stream"}]}], "nbformat": 4, "metadata": {"kernelspec": {"name": "python2", "language": "python", "display_name": "Python 2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 2}, "name": "python", "pygments_lexer": "ipython2", "mimetype": "text/x-python", "version": "2.7.11", "file_extension": ".py", "nbconvert_exporter": "python"}}}